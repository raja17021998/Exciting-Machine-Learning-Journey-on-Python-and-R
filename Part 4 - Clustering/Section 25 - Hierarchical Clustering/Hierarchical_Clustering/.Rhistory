R<-"Python"
R
q()
a<="::"
a<-"j::::"
a
q()
u,<-7
u<-7
u
q()
setwd("E:/Machine Learning Files/Machine Learning A-Z Template Folder/Part 4 - Clustering/Section 25 - Hierarchical Clustering")
setwd("E:/Machine Learning Files/Machine Learning A-Z Template Folder/Part 4 - Clustering/Section 25 - Hierarchical Clustering/Hierarchical_Clustering")
dataset= read.csv('Mall_Customers.csv')
source('E:/Machine Learning Files/Machine Learning A-Z Template Folder/Part 4 - Clustering/Section 25 - Hierarchical Clustering/Hierarchical_Clustering/hc_wd.R', echo=TRUE)
View(X)
View(X)
dendrogram= hclust(dist(X, method = 'eucledian'),
method = 'ward.D')
plot(dendrogram,
main = paste("Dendrogram"),
xlab = 'Customers',
ylab = 'Eucledian Distances')
dataset= read.csv('Mall_Customers.csv')
X= dataset[4:5]
#Using the dendrogram to find optimal number of clusters
# dist: for each pair of customers, the eucledian distance b/w two
# ward.D minimizes within cluster variances
dendrogram= hclust(dist(X, method = 'eucledian'),
method = 'ward.D')
plot(dendrogram,
main = paste("Dendrogram"),
xlab = 'Customers',
ylab = 'Eucledian Distances')
dendrogram = hclust(d = dist(dataset, method = 'euclidean'), method = 'ward.D')
plot(dendrogram,
main = paste('Dendrogram'),
xlab = 'Customers',
ylab = 'Euclidean distances')
source('E:/Machine Learning Files/Machine Learning A-Z Template Folder/Part 4 - Clustering/Section 25 - Hierarchical Clustering/Hierarchical_Clustering/hc_wd.R', echo=TRUE)
hc = hclust(d = dist(dataset, method = 'euclidean'), method = 'ward.D')
y_hc= cutree(hc,5)
y_hc
library(cluster)
clusplot(dataset,
y_hc,
lines = 0,
shade = TRUE,
color = TRUE,
labels = 2,
plotchar = FALSE,
span = TRUE,
main = paste('Clusters of customers'),
xlab = 'Annual Income',
ylab = 'Spending Score')
